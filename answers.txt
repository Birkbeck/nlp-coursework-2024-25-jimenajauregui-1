Answers to the text questions go here.

Part One — Syntax and Style

d- When is the Flesch Kincaid score *not* a valid, robust or reliable estimator of text difficulty? 
Give two conditions. (Text answer, 200 words maximum).

The Flesch-Kincaid Grade Level measures the readability of a text mainly based on sentence and word length. However, this can lead to inaccurate estimations in some cases.
For example, a text with long words might receive a high score, suggesting greater difficulty, but those words could be simple and familiar to native speakers. While Flesch-Kincaid is designed for English, other languages often use longer words to express simple meanings, making the score unreliable in those contexts.
On the other hand, a text with short words and sentences might receive a low score, implying it is easy to read. Yet those words could be highly technical or specific to a particular field, making the text difficult for general readers. It can also be the case that the text has complex grammar or syntactic structures that increase its real difficulty, despite its short sentences or simple words.

Part two - Feature Extraction and Classification

f- Explain your tokenizer function and discuss its performance.

My tokenizer, custom_tokenizer_nltk(text), was developed using NLTK with the aim of cleaning and standardizing text to enhance classification performance.
The function uses word_tokenize to split the text into word-level tokens. Each component of the custom tokenizer was tested individually, and only those that consistently improved classification performance were retained.
All tokens are converted to lowercase to reduce redundancy between tokens that differ only in case and non-alphabetic tokens (such as numbers, punctuation and mixed alphanumerics) are removed to focus on meaningful words.
The tokenizer also applies stopword removal using NLTK’s list of English stopwords. I chose to remove stopwords in the tokenizer itself rather than using the stop_words parameter in the vectorizer, because this gave better classification results.
Very short words (less than 2 characters) are also excluded, as they are unlikely to contribute meaningfully to classification. After testing thresholds, I set the minimum token length to 2 characters.
Finally, WordNetLemmatizer is used to reduce each word to its lemma and to group morphological similiar terms under a single representation (base form). 

This tokenizer was used with the TfidfVectorizer (with max_features=3000, ngram_range=(1, 3), min_df=3) and evaluated with both Random Forest and Linear SVM classifiers.
The best result came from the SVM classifier awhich chieved a macro-average F1 score of 0.627 and macro avg precision of 0.82.
When min_df was not applied, the SVM F1 score slightly dropped to 0.625, but the Random Forest improved from 0.48 to 0.505, showing the best performance for both classifiers.
Using trigrams consistently improved performance across both classifiers.
In contrast, using a spaCy-based tokenizer significantly slowed down training and did not yield better performance.
Other experiments that did not lead to improvements were: apllying POS-specific lemmatization, that was much slower and not beneficial; merging multi-word expressions (like “Prime Minister”) which reduced performance and moving stopword removal to the vectorizer, which slightly improved Random Forest but lowered SVM accuracy.
As a potential improvement, using class_weight='balanced' during training (to address class imbalance) significantly improved results, especially for the underrepresented Liberal Democrat class. With this adjustment, SVM’s F1 score increased significantly to 0.717.
In conclusion, the NLTK-based tokenizer, with lemmatization and stopword removal, achieved the best trade-off between speed and classification performance. It outperformed more complex approaches and worked well within the 3000-feature limit.

