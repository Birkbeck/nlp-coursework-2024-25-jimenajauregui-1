{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jimena/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to /Users/jimena/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Re-assessment template 2025\n",
    "\n",
    "# Note: The template functions here and the dataframe format for structuring your solution is a suggested but not mandatory approach. You can use a different approach if you like, as long as you clearly answer the questions and communicate your answers clearly.\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import cmudict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_novels(path=Path.cwd() / \"p1-texts\" / \"novels\"):\n",
    "    \"\"\"Reads texts from a directory of .txt files and returns a DataFrame with the text, title,\n",
    "    author, and year\"\"\"\n",
    "    novels_dataset = []\n",
    "\n",
    "    for file in path.glob(\"*.txt\"):\n",
    "        name = file.stem\n",
    "        title, author, year = name.split(\"-\")\n",
    "\n",
    "        with open(file, encoding=\"utf-8\") as txt:\n",
    "            text = txt.read()\n",
    "\n",
    "        novels_dataset.append({\n",
    "            \"text\": text,\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"year\": year\n",
    "        })\n",
    "    df = pd.DataFrame(novels_dataset, columns=[\"text\", \"title\", \"author\", \"year\"])\n",
    "\n",
    "    df = df.sort_values(by=\"year\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# df = read_novels()\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}\n",
      "{'Orlando': 0.11753228191839728, 'Portrait_of_the_Artist': 0.10970434538631502, 'Erewhon': 0.09698006527621059, 'Blood_Meridian': 0.08562941459421598, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Black_Moth': 0.07621856866537717, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'The_American': 0.06756581683792795, 'North_and_South': 0.06580904022624899, 'Sense_and_Sensibility': 0.05850947122065384, 'The_Secret_Garden': 0.056573232870154, 'The_Golden_Bowl': 0.04813433820231636}\n"
     ]
    }
   ],
   "source": [
    "def nltk_ttr(text):\n",
    "    \"\"\"Calculates the type-token ratio of a text. Text is tokenized using nltk.word_tokenize.\"\"\"\n",
    "    ttr_dict = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        title = row['title']\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        tokens = [token.lower() for token in tokens if token not in punctuation]\n",
    "\n",
    "        types = set(tokens)\n",
    "        ttr = len(types) / len(tokens)\n",
    "\n",
    "        ttr_dict[title] = ttr\n",
    "\n",
    "    return ttr_dict\n",
    "\n",
    "df = read_novels()\n",
    "ttr_by_novel = nltk_ttr(df)\n",
    "print(ttr_by_novel)\n",
    "#TTR sorted in descending order. Shows grade of lexical diversity within a text.\n",
    "\n",
    "sorted_ttr = dict(sorted(ttr_by_novel.items(), key=lambda item: item[1], reverse=True))\n",
    "print(sorted_ttr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmu_dict = cmudict.dict()\n",
    "\n",
    "def dict_of_syl():\n",
    "    \"\"\"Returns a dictionary mapping words to their syllable counts based on the CMU dictionary\"\"\"\n",
    "    cmu = cmudict.dict()\n",
    "    syl_dict = {}\n",
    "\n",
    "    for word, pronunciations in cmu.items():\n",
    "        # Vowel sounds in CMU have numbers at the end. If that is True count 1 syllable. \n",
    "        syllable_counts = [sum(phoneme[-1].isdigit() for phoneme in pron)\n",
    "        for pron in pronunciations]\n",
    "        # A word may have multiple pronunciations. Get the simplest form.\n",
    "        syl_dict[word.lower()] = min(syllable_counts)\n",
    "\n",
    "    return syl_dict\n",
    "\n",
    "d = dict_of_syl()\n",
    "\n",
    "# print(d['novels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syl(word, d):\n",
    "    \"\"\"Counts the number of syllables in a word given a dictionary of syllables per word.\n",
    "    if the word is not in the dictionary, syllables are estimated by counting vowel clusters\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to count syllables for.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of syllables in the word.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "\n",
    "    if word in d:\n",
    "        return d[word]\n",
    "\n",
    "    # Syllables by counting vowel clusters. 'Y' can sometimes act as a vowel in English\n",
    "    syl_by_vowels = len(re.findall(r'[aeiouy]+', word))\n",
    "\n",
    "    return syl_by_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_level(text, d):\n",
    "    \"\"\"Returns the Flesch-Kincaid Grade Level of a text (higher grade is more difficult).\n",
    "    Requires a dictionary of syllables per word.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        float: The Flesch-Kincaid Grade Level of the text. (higher grade is more difficult)\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Filter out punctuation tokens\n",
    "    words = [word for word in words if re.search(r'\\w', word)]\n",
    "\n",
    "    num_sentences = len(sentences)\n",
    "    num_words = len(words)\n",
    "    # Uses the count_syl function\n",
    "    num_syllables = sum(int(count_syl(word, d)) for word in words)\n",
    "\n",
    "\n",
    "    # According to the Flesch-Kincaid Grade Level formula\n",
    "    fk_grade = 0.39 * (num_words / num_sentences) + 11.8 * (num_syllables / num_words) - 15.59\n",
    "    return fk_grade\n",
    "\n",
    "# text = \"This is an example of the Flesch-Kincaid Grade Level. It estimates the school grade needed to understand a text.\"\n",
    "# fk = fk_level(text, d)\n",
    "# print(fk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nExample:\\ndf = read_novels()\\nd = dict_of_syl()\\nfk_grades = flesch_kincaid(df, d)\\n# Sorted by highest to lowest grade\\nfor title, grade in sorted(fk_grades.items(), key=lambda x: x[1], reverse=True):\\n    print(f\"{title}: {grade:.2f}\")\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flesch_kincaid(df, d):\n",
    "    \"\"\"Returns a dictionary mapping novel titles to their Flesch-Kincaid grade level.\n",
    "    Args:\n",
    "        df: DataFrame including 'title' and 'text' columns.\n",
    "        d: Dictionary of syllables per word.\n",
    "    Returns:\n",
    "        dict: {title: flesch_kincaid_grade}\n",
    "    \"\"\"\n",
    "    fk_grade = {}\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        title = row['title']\n",
    "        text = row['text']\n",
    "        fk_grade[title] = fk_level(text, d)\n",
    "\n",
    "    return fk_grade\n",
    "\n",
    "\"\"\"\"\n",
    "Example:\n",
    "df = read_novels()\n",
    "d = dict_of_syl()\n",
    "fk_grades = flesch_kincaid(df, d)\n",
    "# Sorted by highest to lowest grade\n",
    "for title, grade in sorted(fk_grades.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{title}: {grade:.2f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximun text length: 1158935 characters\n"
     ]
    }
   ],
   "source": [
    "# Check the longest text length in our dataset\n",
    "df[\"text_length\"] = df[\"text\"].apply(len)\n",
    "max_length = df[\"text_length\"].max()\n",
    "print(f\"Maximun text length: {max_length} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Check printing for longest text\\nlongest_text = df.loc[df[\"text_length\"].idxmax(), \"text\"]\\nparsed_lt = nlp(longest_text)\\nprint(f\"Longest text number of tokens: {len(parsed_lt)}\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse(df, store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    \"\"\"Parses the text of a DataFrame using spaCy, stores the parsed docs as a column and writes \n",
    "    the resulting  DataFrame to a pickle file\"\"\"\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    max_length = df[\"text\"].apply(len).max()\n",
    "\n",
    "    # Add nlp.max_length after cheking the longest text in the dataset (1158935 char) nlp.max_length default 1000000.\n",
    "    if max_length > nlp.max_length:\n",
    "        nlp.max_length = max_length + 500\n",
    "    \n",
    "    df[\"parsed\"] = df[\"text\"].apply(nlp)\n",
    "    \n",
    "    pickle_path = store_path / out_name\n",
    "    with open(pickle_path, \"wb\") as f:\n",
    "        pickle.dump(df, f)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\" Check printing for longest text\n",
    "longest_text = df.loc[df[\"text_length\"].idxmax(), \"text\"]\n",
    "parsed_lt = nlp(longest_text)\n",
    "print(f\"Longest text number of tokens: {len(parsed_lt)}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_path = Path.cwd() / \"pickles\"\n",
    "df = parse(df)  \n",
    "df = pd.read_pickle(store_path / \"parsed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict_of_syl()\n",
    "\n",
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], d), 4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_objects(doc, n=10):\n",
    "    \"\"\"Returns a list of the ten most common syntactic objects overall in the text\"\"\"\n",
    "    objects = [token.text.lower() for token in doc if token.dep_ == \"dobj\"]\n",
    "    most_common_objects = Counter(objects).most_common(n)\n",
    "    return most_common_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects_by_verb_count(doc, verb = \"hear\", n=10 ):\n",
    "    \"\"\"Returns a list of the ten most common syntactic subjects of a verb (in any tense) in the text, ordered by their frequency.\"\"\"\n",
    "    verb_subjects = []\n",
    "    for token in doc:\n",
    "        # check if it is a verb with lemma \"hear\".\n",
    "        if token.lemma_ == verb and token.pos_ == \"VERB\":\n",
    "            for child in token.children:\n",
    "                if child.dep_ == \"nsubj\":\n",
    "                    verb_subjects.append(child.text.lower())\n",
    "    common_verb_subjects = Counter(verb_subjects).most_common(n)\n",
    "    return common_verb_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjective_counts(doc):\n",
    "    \"\"\"Extracts the most common adjectives in a parsed document. Returns a list of tuples.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jimena/NLP/nlp-coursework-2024-25-jimenajauregui-1/p1-texts/novels\n",
      "                                                text                  title  \\\n",
      "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...  Sense_and_Sensibility   \n",
      "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North_and_South   \n",
      "2  Book the First--Recalled to Life\\n\\n\\n\\n\\nI. T...   A_Tale_of_Two_Cities   \n",
      "3  SAMUEL BUTLER.\\nAugust 7, 1901\\n\\nCHAPTER I: W...                Erewhon   \n",
      "4  THE AMERICAN\\n\\nby Henry James\\n\\n\\n1877\\n\\n\\n...           The_American   \n",
      "\n",
      "    author  year  \n",
      "0   Austen  1811  \n",
      "1  Gaskell  1855  \n",
      "2  Dickens  1858  \n",
      "3   Butler  1872  \n",
      "4    James  1877  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/jimena/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text                  title  \\\n",
      "0  \\nCHAPTER 1\\n\\nThe family of Dashwood had long...  Sense_and_Sensibility   \n",
      "1  'Wooed and married and a'.'\\n'Edith!' said Mar...        North_and_South   \n",
      "2  Book the First--Recalled to Life\\n\\n\\n\\n\\nI. T...   A_Tale_of_Two_Cities   \n",
      "3  SAMUEL BUTLER.\\nAugust 7, 1901\\n\\nCHAPTER I: W...                Erewhon   \n",
      "4  THE AMERICAN\\n\\nby Henry James\\n\\n\\n1877\\n\\n\\n...           The_American   \n",
      "\n",
      "    author  year                                             parsed  \n",
      "0   Austen  1811  (\\n, CHAPTER, 1, \\n\\n, The, family, of, Dashwo...  \n",
      "1  Gaskell  1855  (', Wooed, and, married, and, a, ', ., ', \\n, ...  \n",
      "2  Dickens  1858  (Book, the, First, --, Recalled, to, Life, \\n\\...  \n",
      "3   Butler  1872  (SAMUEL, BUTLER, ., \\n, August, 7, ,, 1901, \\n...  \n",
      "4    James  1877  (THE, AMERICAN, \\n\\n, by, Henry, James, \\n\\n\\n...  \n",
      "{'Sense_and_Sensibility': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'North_and_South': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'A_Tale_of_Two_Cities': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'Erewhon': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'The_American': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'Dorian_Gray': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'Tess_of_the_DUrbervilles': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'The_Golden_Bowl': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'The_Secret_Garden': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'Portrait_of_the_Artist': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'The_Black_Moth': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'Orlando': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}, 'Blood_Meridian': {'Sense_and_Sensibility': 0.05850947122065384, 'North_and_South': 0.06580904022624899, 'A_Tale_of_Two_Cities': 0.07287318093056312, 'Erewhon': 0.09698006527621059, 'The_American': 0.06756581683792795, 'Dorian_Gray': 0.08498209005457125, 'Tess_of_the_DUrbervilles': 0.08015054009113641, 'The_Golden_Bowl': 0.04813433820231636, 'The_Secret_Garden': 0.056573232870154, 'Portrait_of_the_Artist': 0.10970434538631502, 'The_Black_Moth': 0.07621856866537717, 'Orlando': 0.11753228191839728, 'Blood_Meridian': 0.08562941459421598}}\n",
      "{'Sense_and_Sensibility': 11.1494, 'North_and_South': 7.0272, 'A_Tale_of_Two_Cities': 10.1137, 'Erewhon': 14.8484, 'The_American': 8.2488, 'Dorian_Gray': 5.0382, 'Tess_of_the_DUrbervilles': 7.8521, 'The_Golden_Bowl': 12.5918, 'The_Secret_Garden': 4.6748, 'Portrait_of_the_Artist': 6.5356, 'The_Black_Moth': 4.3053, 'Orlando': 9.6777, 'Blood_Meridian': 5.558}\n",
      "Sense_and_Sensibility\n",
      "[('her', 397), ('it', 380), ('him', 279), ('you', 214), ('what', 197), ('them', 191), ('me', 177), ('which', 114), ('herself', 94), ('nothing', 79)]\n",
      "\n",
      "\n",
      "North_and_South\n",
      "[('it', 556), ('him', 450), ('her', 436), ('what', 352), ('me', 345), ('you', 277), ('them', 184), ('which', 170), ('herself', 146), ('margaret', 120)]\n",
      "\n",
      "\n",
      "A_Tale_of_Two_Cities\n",
      "[('it', 451), ('him', 409), ('you', 240), ('me', 232), ('her', 196), ('them', 167), ('what', 153), ('himself', 132), ('that', 111), ('hand', 92)]\n",
      "\n",
      "\n",
      "Erewhon\n",
      "[('it', 230), ('me', 221), ('them', 158), ('which', 151), ('him', 104), ('what', 101), ('that', 81), ('her', 48), ('myself', 47), ('us', 40)]\n",
      "\n",
      "\n",
      "The_American\n",
      "[('it', 583), ('you', 479), ('me', 393), ('him', 369), ('her', 306), ('what', 292), ('that', 182), ('them', 149), ('something', 114), ('_', 105)]\n",
      "\n",
      "\n",
      "Dorian_Gray\n",
      "[('it', 300), ('me', 256), ('him', 252), ('you', 188), ('that', 178), ('what', 166), ('them', 98), ('her', 92), ('something', 52), ('himself', 50)]\n",
      "\n",
      "\n",
      "Tess_of_the_DUrbervilles\n",
      "[('her', 496), ('it', 425), ('him', 294), ('me', 245), ('you', 239), ('what', 185), ('them', 171), ('that', 128), ('which', 121), ('herself', 90)]\n",
      "\n",
      "\n",
      "The_Golden_Bowl\n",
      "[('it', 1174), ('her', 700), ('what', 654), ('him', 543), ('them', 307), ('me', 246), ('you', 237), ('that', 224), ('herself', 183), ('nothing', 125)]\n",
      "\n",
      "\n",
      "The_Secret_Garden\n",
      "[('it', 368), ('him', 220), ('her', 168), ('what', 135), ('me', 110), ('them', 94), ('you', 89), ('things', 76), ('that', 61), ('mary', 57)]\n",
      "\n",
      "\n",
      "Portrait_of_the_Artist\n",
      "[('him', 295), ('it', 180), ('them', 103), ('what', 102), ('you', 91), ('me', 75), ('that', 57), ('which', 56), ('himself', 49), ('eyes', 47)]\n",
      "\n",
      "\n",
      "The_Black_Moth\n",
      "[('him', 404), ('you', 359), ('it', 344), ('me', 331), ('her', 270), ('what', 116), ('that', 102), ('_', 98), ('hand', 92), ('himself', 83)]\n",
      "\n",
      "\n",
      "Orlando\n",
      "[('it', 192), ('her', 162), ('him', 149), ('them', 94), ('which', 85), ('what', 70), ('that', 46), ('himself', 41), ('herself', 41), ('us', 39)]\n",
      "\n",
      "\n",
      "Blood_Meridian\n",
      "[('it', 415), ('him', 289), ('them', 288), ('what', 110), ('head', 91), ('horse', 85), ('horses', 79), ('pistol', 64), ('man', 62), ('you', 53)]\n",
      "\n",
      "\n",
      "Sense_and_Sensibility\n",
      "[('i', 28), ('you', 19), ('she', 14), ('elinor', 6), ('he', 6), ('they', 5), ('me', 4), ('jennings', 3), ('brandon', 1), ('us', 1)]\n",
      "\n",
      "\n",
      "North_and_South\n",
      "[('she', 59), ('i', 42), ('he', 23), ('you', 15), ('they', 12), ('margaret', 10), ('me', 5), ('we', 5), ('thornton', 3), ('who', 3)]\n",
      "\n",
      "\n",
      "A_Tale_of_Two_Cities\n",
      "[('i', 21), ('he', 18), ('you', 12), ('she', 10), ('they', 5), ('monseigneur', 2), ('me', 2), ('one', 1), ('jerry', 1), ('we', 1)]\n",
      "\n",
      "\n",
      "Erewhon\n",
      "[('i', 38), ('he', 4), ('they', 3), ('she', 2), ('we', 1), ('who', 1), ('destruction', 1), ('machines', 1), ('one', 1)]\n",
      "\n",
      "\n",
      "The_American\n",
      "[('he', 17), ('i', 13), ('you', 10), ('she', 5), ('newman', 4), ('they', 2), ('we', 2), ('who', 1), ('me', 1), ('one', 1)]\n",
      "\n",
      "\n",
      "Dorian_Gray\n",
      "[('i', 24), ('he', 16), ('one', 3), ('you', 3), ('lovers', 1), ('hast', 1), ('who', 1), ('jars', 1), ('dorian', 1)]\n",
      "\n",
      "\n",
      "Tess_of_the_DUrbervilles\n",
      "[('she', 37), ('i', 20), ('they', 12), ('you', 8), ('he', 6), ('who', 6), ('tess', 4), ('clare', 3), ('lady', 2), ('queen', 1)]\n",
      "\n",
      "\n",
      "The_Golden_Bowl\n",
      "[('she', 16), ('he', 6), ('you', 5), ('maggie', 2), ('him', 2), ('man', 1), ('they', 1), ('who', 1), ('amerigo', 1), ('which', 1)]\n",
      "\n",
      "\n",
      "The_Secret_Garden\n",
      "[('i', 28), ('she', 25), ('he', 16), ('you', 8), ('we', 6), ('mary', 3), ('lennox', 2), ('they', 2), ('colin', 2), ('one', 2)]\n",
      "\n",
      "\n",
      "Portrait_of_the_Artist\n",
      "[('he', 61), ('you', 12), ('i', 8), ('stephen', 5), ('who', 2), ('they', 1), ('boy', 1), ('burst', 1), ('listener', 1), ('that', 1)]\n",
      "\n",
      "\n",
      "The_Black_Moth\n",
      "[('i', 37), ('he', 5), ('you', 5), ('we', 4), ('richard', 2), ('she', 2), ('street', 1), (\"ye've\", 1), ('bettison', 1), ('madam', 1)]\n",
      "\n",
      "\n",
      "Orlando\n",
      "[('she', 21), ('he', 7), ('orlando', 4), ('one', 3), ('they', 2), ('i', 2), ('god', 1), ('we', 1), ('that', 1), ('which', 1)]\n",
      "\n",
      "\n",
      "Blood_Meridian\n",
      "[('he', 25), ('they', 20), ('you', 5), ('i', 5), ('who', 3), ('she', 2), ('all', 2), ('we', 2), ('man', 2), ('nobody', 1)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    uncomment the following lines to run the functions once you have completed them\n",
    "    \"\"\"\n",
    "    path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    print(path)\n",
    "    df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    print(df.head())\n",
    "    nltk.download(\"cmudict\")\n",
    "    parse(df)\n",
    "    print(df.head())\n",
    "    print(get_ttrs(df))\n",
    "    print(get_fks(df))\n",
    "    df = pd.read_pickle(Path.cwd() / \"pickles\" /\"parsed.pickle\")\n",
    "    # print(adjective_counts(df))\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(common_objects(row[\"parsed\"], 10))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_count(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \"\"\"\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
