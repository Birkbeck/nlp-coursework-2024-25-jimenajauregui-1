{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-assessment template 2025\n",
    "\n",
    "# Note: The template functions here and the dataframe format for structuring your solution is a suggested but not mandatory approach. You can use a different approach if you like, as long as you clearly answer the questions and communicate your answers clearly.\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_level(text, d):\n",
    "    \"\"\"Returns the Flesch-Kincaid Grade Level of a text (higher grade is more difficult).\n",
    "    Requires a dictionary of syllables per word.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        float: The Flesch-Kincaid Grade Level of the text. (higher grade is more difficult)\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syl(word, d):\n",
    "    \"\"\"Counts the number of syllables in a word given a dictionary of syllables per word.\n",
    "    if the word is not in the dictionary, syllables are estimated by counting vowel clusters\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to count syllables for.\n",
    "        d (dict): A dictionary of syllables per word.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of syllables in the word.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  \\\n",
      "0   \\nCHAPTER 1\\n\\nThe family of Dashwood had long...   \n",
      "1   'Wooed and married and a'.'\\n'Edith!' said Mar...   \n",
      "2   Book the First--Recalled to Life\\n\\n\\n\\n\\nI. T...   \n",
      "3   SAMUEL BUTLER.\\nAugust 7, 1901\\n\\nCHAPTER I: W...   \n",
      "4   THE AMERICAN\\n\\nby Henry James\\n\\n\\n1877\\n\\n\\n...   \n",
      "5   \\nThe Picture of Dorian Gray\\n\\nby\\n\\nOscar Wi...   \n",
      "6   Phase the First: The Maiden\\n\\n\\nI\\n\\n\\nOn an ...   \n",
      "7   BOOK FIRST: THE PRINCE\\n\\n\\n\\n\\nPART FIRST\\n\\n...   \n",
      "8   THE SECRET GARDEN\\n\\nBY FRANCES HODGSON BURNET...   \n",
      "9   Chapter 1\\n\\nOnce upon a time and a very good ...   \n",
      "10  \\nTHE BLACK MOTH\\n\\nA ROMANCE OF THE XVIII CEN...   \n",
      "11  ORLANDO\\n\\nA BIOGRAPHY\\n\\nBY\\n\\nVIRGINIA WOOLF...   \n",
      "12  Your ideas are terrifying and your hearts are ...   \n",
      "\n",
      "                       title    author  year  \n",
      "0      Sense_and_Sensibility    Austen  1811  \n",
      "1            North_and_South   Gaskell  1855  \n",
      "2       A_Tale_of_Two_Cities   Dickens  1858  \n",
      "3                    Erewhon    Butler  1872  \n",
      "4               The_American     James  1877  \n",
      "5                Dorian_Gray     Wilde  1890  \n",
      "6   Tess_of_the_DUrbervilles     Hardy  1891  \n",
      "7            The_Golden_Bowl     James  1904  \n",
      "8          The_Secret_Garden   Burnett  1911  \n",
      "9     Portrait_of_the_Artist     Joyce  1916  \n",
      "10            The_Black_Moth     Heyer  1926  \n",
      "11                   Orlando     Woolf  1928  \n",
      "12            Blood_Meridian  McCarthy  1930  \n"
     ]
    }
   ],
   "source": [
    "def read_novels(path=Path.cwd() / \"p1-texts\" / \"novels\"):\n",
    "    \"\"\"Reads texts from a directory of .txt files and returns a DataFrame with the text, title,\n",
    "    author, and year\"\"\"\n",
    "    novels_dataset = []\n",
    "\n",
    "    for file in path.glob(\"*.txt\"):\n",
    "        name = file.stem\n",
    "        title, author, year = name.split(\"-\")\n",
    "\n",
    "        with open(file, encoding=\"utf-8\") as txt:\n",
    "            text = txt.read()\n",
    "\n",
    "        novels_dataset.append({\n",
    "            \"text\": text,\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"year\": year\n",
    "        })\n",
    "    df = pd.DataFrame(novels_dataset, columns=[\"text\", \"title\", \"author\", \"year\"])\n",
    "\n",
    "    df = df.sort_values(by=\"year\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "novels = read_novels()\n",
    "print(novels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(df, store_path=Path.cwd() / \"pickles\", out_name=\"parsed.pickle\"):\n",
    "    \"\"\"Parses the text of a DataFrame using spaCy, stores the parsed docs as a column and writes \n",
    "    the resulting  DataFrame to a pickle file\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_ttr(text):\n",
    "    \"\"\"Calculates the type-token ratio of a text. Text is tokenized using nltk.word_tokenize.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ttrs(df):\n",
    "    \"\"\"helper function to add ttr to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = nltk_ttr(row[\"text\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fks(df):\n",
    "    \"\"\"helper function to add fk scores to a dataframe\"\"\"\n",
    "    results = {}\n",
    "    cmudict = nltk.corpus.cmudict.dict()\n",
    "    for i, row in df.iterrows():\n",
    "        results[row[\"title\"]] = round(fk_level(row[\"text\"], cmudict), 4)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects_by_verb_pmi(doc, target_verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subjects_by_verb_count(doc, verb):\n",
    "    \"\"\"Extracts the most common subjects of a given verb in a parsed document. Returns a list.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjective_counts(doc):\n",
    "    \"\"\"Extracts the most common adjectives in a parsed document. Returns a list of tuples.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    uncomment the following lines to run the functions once you have completed them\n",
    "    \"\"\"\n",
    "    #path = Path.cwd() / \"p1-texts\" / \"novels\"\n",
    "    #print(path)\n",
    "    #df = read_novels(path) # this line will fail until you have completed the read_novels function above.\n",
    "    #print(df.head())\n",
    "    #nltk.download(\"cmudict\")\n",
    "    #parse(df)\n",
    "    #print(df.head())\n",
    "    #print(get_ttrs(df))\n",
    "    #print(get_fks(df))\n",
    "    #df = pd.read_pickle(Path.cwd() / \"pickles\" /\"name.pickle\"\n",
    "    # print(adjective_counts(df))\n",
    "    \"\"\" \n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_count(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        print(row[\"title\"])\n",
    "        print(subjects_by_verb_pmi(row[\"parsed\"], \"hear\"))\n",
    "        print(\"\\n\")\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
